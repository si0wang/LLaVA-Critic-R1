import argparse
import os
import json
import math

import cv2
import numpy as np
from PIL import Image
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoProcessor
from vllm import LLM, SamplingParams
import random

instruct_prompt = (
    r"You FIRST think about the reasoning process as an internal monologue "
    r"and then provide the final answer. The reasoning process MUST BE enclosed "
    r"within <think> </think> tags. The final answer MUST BE put in \boxed{}."
)


def dump_to_jsonl(obj: list[dict], path: str):
    """Write a list of dicts to a JSONL file."""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        for entry in obj:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")


def split_list(lst: list, n: int) -> list[list]:
    """Split a list into n (roughly) equal-sized chunks."""
    chunk_size = math.ceil(len(lst) / n)
    return [lst[i : i + chunk_size] for i in range(0, len(lst), chunk_size)]

def make_conv_rm(data_obj):
    random_number = random.choice([0, 1])
    if random_number == 0:
        answers = [data_obj["chosen"], data_obj["rejected"]]
    else:
        answers = [data_obj["rejected"], data_obj["chosen"]]
    prompt_template = (
        "You are a highly skilled and impartial evaluator tasked with comparing two responses generated by a Large Multimodal Model for a given question. "
        "- Start with a thorough, side-by-side comparative analysis enclosed within <think> and </think> tags. A tie is not permitted; you must choose a better option.\n\n"
        "- Conclude with a single numeric choice enclosed within <answer> and </answer> tags:\n"
        "  - Output \"1\" if Response 1 is better.\n"
        "  - Output \"2\" if Response 2 is better.\n\n"
        "###### **Input:**  \n"
        "###### [Question]:\n{question}  \n\n"
        "###### [Response 1]:\n{answer1}  \n\n"
        "###### [Response 2]:\n{answer2}  \n\n"
        "###### **Output Format (strictly follow):**  \n"
        "<think>Your detailed comparative analysis goes here</think><answer>1/2</answer>"
    )
    formatted_prompt = prompt_template.format(question=data_obj["prompt"], answer1=answers[0], answer2=answers[1])
    return formatted_prompt, random_number

def build_prompt(data_obj):
    random_number = random.choice([0, 1])
    if random_number == 0:
        answers = [data_obj["chosen"], data_obj["rejected"]]
    else:
        answers = [data_obj["rejected"], data_obj["chosen"]]
    prompt_str = f''' You are a highly capable multimodal AI assistant tasked with evaluating answers to visual questions. Please analyze the following image and question, then determine which of the two provided answers is better.

Question: {data_obj["prompt"]}

Answer 1: {answers[0]}

Answer 2: {answers[1]}

Please evaluate both answers based on the following criteria:
1. Accuracy: How well does the answer align with the visual information in the image?
2. Completeness: Does the answer fully address all aspects of the question?
3. Clarity: Is the answer easy to understand and well-articulated?
4. Relevance: Does the answer directly relate to the question and the image?

After your evaluation, please:
1. Explain your reasoning for each criterion.
2. Provide an overall judgment on which answer is better (Answer 1 or Answer 2). For example: Overall Judgment: Answer X is better.

Your response should be structured and detailed, demonstrating your understanding of both the visual and textual elements of the task.'''
    return prompt_str, random_number


def sample_video_frames(video_datas) -> list[Image.Image]:
    """
    Open a video file and uniformly sample `num_frames` frames.
    Returns a list of PIL Images.
    """
    frames = []
    for data in video_datas:
        frame_path = os.path.join('/fsx-project/xywang96/ThinkLite-VL/MM-RLHF-RewardBench/images', data)
        frame = Image.open(frame_path).convert('RGB')
        frames.append(frame)
    return frames


def eval_model(args):
    # Initialize the multimodal LLM
    model = LLM(
        model=args.model_id,
        limit_mm_per_prompt={"image": 10, "video": 10},
        tensor_parallel_size=8,
        disable_mm_preprocessor_cache=True,
        gpu_memory_utilization=0.70, 
        max_num_seqs=4,              
        max_num_batched_tokens=8192, 
        max_model_len=8192,   
    )
    processor = AutoProcessor.from_pretrained(args.model_id)
    sampling_params = SamplingParams(temperature=0, max_tokens=args.max_tokens)

    # Load dataset (expects fields 'question', 'image', 'video', and optionally 'id')
    dataset = list(load_dataset("yifanzhang114/MM-RLHF-RewardBench", split="train"))
    final_responses = []

    # Process in batches
    chunks = split_list(dataset, math.ceil(len(dataset) / args.batch_size))
    for chunk in tqdm(chunks, desc="Evaluating batches"):
        batch_inputs = []
        for data in chunk:
            if data.get('video'):
                frames = sample_video_frames(data['video'])
                mm_spec = {"type": "video", "min_frames": 10, "max_frames": 32}
                mm_data = {"video": frames}
            else:
                image_path = os.path.join('/fsx-project/xywang96/ThinkLite-VL/MM-RLHF-RewardBench/images', data['image'])
                img = Image.open(image_path).convert('RGB')
                if img.height < 28 or img.width < 28:
                    print(f"Skip {data.get('id', data.get('image'))}: image too small")
                    continue
                mm_spec = {
                    "type": "image",
                    "min_pixels": 224 * 224,
                    "max_pixels": 1280 * 28 * 28,
                }
                mm_data = {"image": img}

            if args.model_id == 'yifanzhang114/R1-Reward':
                qs, number = make_conv_rm(data)
            else:
                qs, number = build_prompt(data)
                if args.is_thinking:
                    qs += instruct_prompt
            data['number'] = number

            message = [
                {"role": "system", "content": "You are a helpful assistant."},
                {
                    "role": "user",
                    "content": [
                        mm_spec,
                        {"type": "text", "text": qs},
                    ],
                },
            ]
            prompt = processor.apply_chat_template(
                message, tokenize=False, add_generation_prompt=True
            )

            if args.is_mimo_thinking:
                prompt += "<think>"

            batch_inputs.append({"prompt": prompt, "multi_modal_data": mm_data})

        # Generate and collect responses
        outputs = model.generate(batch_inputs, sampling_params=sampling_params, use_tqdm=False)
        for out, data in zip(outputs, chunk):
            data['response'] = out.outputs[0].text
            final_responses.append(data)

    # Save all responses
    dump_to_jsonl(final_responses, args.answers_file)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_id",     type=str, required=True,
                        help="VLLM model ID (e.g., 'qwen/...')")
    parser.add_argument("--answers-file", type=str, required=True,
                        help="Path to write out JSONL of answers")
    parser.add_argument("--batch-size",   type=int, default=128,
                        help="Number of examples per batch")
    parser.add_argument("--max-tokens",   type=int, default=2048,
                        help="Maximum new tokens to generate")
    parser.add_argument("--is-thinking", action="store_true",
                        help="Append the thinking prompt to the question")
    parser.add_argument("--is-mimo-thinking", action="store_true",
                        help="Append an explicit <think> tag after the chat template")
    args = parser.parse_args()

    eval_model(args)
